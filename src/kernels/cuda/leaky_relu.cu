#include "kernels/cuda/cuda_utils.cuh"
#include "kernels/leaky_relu.h"
#include <cuda_runtime.h>
#include <stdarg.h>

__global__ void leaky_relu_cuda_forward_float_contig_kernel(const float *a,
                                                            float *c,
                                                            u64 num_elements,
                                                            float alpha) {
  u64 idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < num_elements) {
    c[idx] = a[idx] > 0 ? a[idx] : a[idx] * alpha;
  }
}

__global__ void leaky_relu_cuda_backward_float_contig_kernel(const float *dout,
                                                             const float *a,
                                                             float *da,
                                                             u64 num_elements,
                                                             float alpha) {
  u64 idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < num_elements) {
    if (da) {
      float grad_multiplier = a[idx] > 0 ? 1.0f : alpha;
      da[idx] += dout[idx] * grad_multiplier;
    }
  }
}

__global__ void leaky_relu_cuda_forward_float_non_contig_kernel(
    const float *a_data, const u64 *a_strides, float *c_data,
    const u64 *c_strides, const u64 *shape, u64 ndim, u64 num_elements,
    float alpha) {
  u64 linear_idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (linear_idx < num_elements) {
    u64 coords[MAX_NDIM];
    cuda_linear_to_coords(linear_idx, shape, ndim, coords);
    u64 a_offset = cuda_get_offset(coords, a_strides, ndim);
    u64 c_offset = cuda_get_offset(coords, c_strides, ndim);
    c_data[c_offset] =
        a_data[a_offset] > 0 ? a_data[a_offset] : a_data[a_offset] * alpha;
  }
}

__global__ void leaky_relu_cuda_backward_float_non_contig_kernel(
    const float *dout_data, const u64 *dout_strides, const float *a_data,
    const u64 *a_strides, float *da_data, const u64 *da_strides,
    const u64 *shape, u64 ndim, u64 num_elements, float alpha) {
  u64 linear_idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (linear_idx < num_elements) {
    u64 coords[MAX_NDIM];
    cuda_linear_to_coords(linear_idx, shape, ndim, coords);
    u64 dout_offset = cuda_get_offset(coords, dout_strides, ndim);
    u64 a_offset = cuda_get_offset(coords, a_strides, ndim);
    u64 da_offset = cuda_get_offset(coords, da_strides, ndim);

    if (da_data) {
      float grad_multiplier = a_data[a_offset] > 0 ? 1.0f : alpha;
      da_data[da_offset] += dout_data[dout_offset] * grad_multiplier;
    }
  }
}

void leaky_relu_cuda_forward(const Tensor **inputs, Tensor *output, ...) {
  const Tensor *a = inputs[0];
  va_list args;
  va_start(args, output);
  float alpha = (float)va_arg(args, double);
  va_end(args);

  u64 num_elements = numel(a);

  int block_size = 256;
  int grid_size = (num_elements + block_size - 1) / block_size;

  if (is_contiguous(a) && is_contiguous(output)) {
    switch (a->dtype) {
    case FLOAT32:
      leaky_relu_cuda_forward_float_contig_kernel<<<grid_size, block_size>>>(
          (const float *)a->data, (float *)output->data, num_elements, alpha);
      break;
    default:
      break;
    }
  } else {
    switch (a->dtype) {
    case FLOAT32:
      leaky_relu_cuda_forward_float_non_contig_kernel<<<grid_size,
                                                        block_size>>>(
          (const float *)a->data, a->strides, (float *)output->data,
          output->strides, a->shape, a->ndim, num_elements, alpha);
      break;
    default:
      break;
    }
  }
  cudaDeviceSynchronize();
}

void leaky_relu_cuda_backward(Tensor **inputs, const Tensor *output, ...) {
  const Tensor *a = inputs[0];
  va_list args;
  va_start(args, output);
  float alpha = (float)va_arg(args, double);
  va_end(args);

  u64 num_elements = numel(a);

  int block_size = 256;
  int grid_size = (num_elements + block_size - 1) / block_size;

  if (is_contiguous(a) && is_contiguous(output)) {
    switch (a->dtype) {
    case FLOAT32:
      leaky_relu_cuda_backward_float_contig_kernel<<<grid_size, block_size>>>(
          (const float *)output->grad->data, (const float *)a->data,
          a->requires_grad ? (float *)a->grad->data : NULL, num_elements,
          alpha);
      break;
    default:
      break;
    }
  } else {
    switch (a->dtype) {
    case FLOAT32:
      leaky_relu_cuda_backward_float_non_contig_kernel<<<grid_size,
                                                         block_size>>>(
          (const float *)output->grad->data, output->grad->strides,
          (const float *)a->data, a->strides,
          a->requires_grad ? (float *)a->grad->data : NULL,
          a->requires_grad ? a->grad->strides : NULL, a->shape, a->ndim,
          num_elements, alpha);
      break;
    default:
      break;
    }
  }
  cudaDeviceSynchronize();
}

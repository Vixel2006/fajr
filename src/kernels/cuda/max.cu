#include "kernels/cuda/cuda_utils.cuh"
#include "kernels/max.h"
#include "tensor.h"
#include <cuda_runtime.h>
#include <float.h>
#include <stdarg.h>

template <u64 block_size>
__device__ void warp_reduce(volatile float *sdata, u64 tid) {
  if (block_size >= 64)
    sdata[tid] = fmaxf(sdata[tid], sdata[tid + 32]);
  if (block_size >= 32)
    sdata[tid] = fmaxf(sdata[tid], sdata[tid + 16]);
  if (block_size >= 16)
    sdata[tid] = fmaxf(sdata[tid], sdata[tid + 8]);
  if (block_size >= 8)
    sdata[tid] = fmaxf(sdata[tid], sdata[tid + 4]);
  if (block_size >= 4)
    sdata[tid] = fmaxf(sdata[tid], sdata[tid + 2]);
  if (block_size >= 2)
    sdata[tid] = fmaxf(sdata[tid], sdata[tid + 1]);
}

template <u64 block_size>
__global__ void max_cuda_forward_float_contig_kernel(const float *a, float *c,
                                                     u64 num_elements) {
  extern __shared__ float sdata[];
  u64 tid = threadIdx.x;
  u64 i = (blockDim.x * 2) * blockIdx.x + threadIdx.x;
  u64 grid_size = blockDim.x * 2 * gridDim.x;

  float thread_max_val = FLT_MIN;
  while (i < num_elements) {
    float val1 = (i < num_elements) ? a[i] : FLT_MIN;
    float val2 = (i + blockDim.x < num_elements) ? a[i + blockDim.x] : FLT_MIN;
    thread_max_val = fmaxf(thread_max_val, fmaxf(val1, val2));
    i += grid_size;
  }
  sdata[tid] = thread_max_val;
  __syncthreads();

  for (u64 s = blockDim.x / 2; s > 32; s >>= 1) {
    if (tid < s)
      sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
    __syncthreads();
  }

  if (tid < 32)
    warp_reduce<block_size>(sdata, tid);

  if (tid == 0)
    c[0] = sdata[0];
}

template <u64 block_size>
__global__ void
max_cuda_forward_float_noncontig_kernel(const float *a_data, const u64 *a_shape,
                                        const u64 *a_strides, u64 a_ndim,
                                        float *c, u64 num_elements) {
  extern __shared__ float sdata[];
  u64 tid = threadIdx.x;
  u64 i = (blockDim.x * 2) * blockIdx.x + threadIdx.x;
  u64 grid_size = blockDim.x * 2 * gridDim.x;

  u64 coords_i[MAX_NDIM];
  u64 coords_i_plus_blockdim[MAX_NDIM];

  float max_val = FLT_MIN;
  while (i < num_elements) {
    float val1 = FLT_MIN;
    if (i < num_elements) {
      cuda_linear_to_coords(i, a_shape, a_ndim, coords_i);
      u64 offset_i = cuda_get_offset(coords_i, a_strides, a_ndim);
      val1 = a_data[offset_i];
    }

    float val2 = FLT_MIN;
    if (i + blockDim.x < num_elements) {
      cuda_linear_to_coords(i + blockDim.x, a_shape, a_ndim,
                            coords_i_plus_blockdim);
      u64 offset_i_plus_blockdim =
          cuda_get_offset(coords_i_plus_blockdim, a_strides, a_ndim);
      val2 = a_data[offset_i_plus_blockdim];
    }
    max_val = fmaxf(max_val, fmaxf(val1, val2));
    i += grid_size;
  }
  sdata[tid] = max_val;
  __syncthreads();

  for (u64 s = blockDim.x / 2; s > 32; s >>= 1) {
    if (tid < s)
      sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
    __syncthreads();
  }

  if (tid < 32)
    warp_reduce<block_size>(sdata, tid);

  if (tid == 0)
    c[0] = sdata[0];
}

__global__ void max_cuda_forward_float_dim_contig_kernel(
    const float *a_data, const u64 *a_shape, const u64 *a_strides, u64 a_ndim,
    float *c_data, const u64 *c_shape, const u64 *c_strides, u64 c_ndim,
    u64 dim, u64 reduce_size, bool keepdim) {
  u64 output_idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (output_idx >= cuda_numel_from_shape(c_shape, c_ndim)) {
    return;
  }

  u64 output_coords[MAX_NDIM];
  cuda_linear_to_coords(output_idx, c_shape, c_ndim, output_coords);

  float max_val = FLT_MIN;
  u64 input_coords[MAX_NDIM];
  for (u64 i = 0; i < reduce_size; ++i) {
    for (u64 d = 0; d < a_ndim; ++d) {
      if (d == dim) {
        input_coords[d] = i;
      } else {
        input_coords[d] = output_coords[d > dim && !keepdim ? d - 1 : d];
      }
    }
    u64 input_offset = cuda_get_offset(input_coords, a_strides, a_ndim);
    max_val = fmaxf(max_val, a_data[input_offset]);
  }
  u64 output_offset = cuda_get_offset(output_coords, c_strides, c_ndim);
  c_data[output_offset] = max_val;
}

__global__ void max_cuda_forward_float_dim_noncontig_kernel(
    const float *a_data, const u64 *a_shape, const u64 *a_strides, u64 a_ndim,
    float *c_data, const u64 *c_shape, const u64 *c_strides, u64 c_ndim,
    u64 dim, u64 reduce_size, bool keepdim) {
  u64 output_idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (output_idx >= cuda_numel_from_shape(c_shape, c_ndim)) {
    return;
  }

  u64 output_coords[MAX_NDIM];
  cuda_linear_to_coords(output_idx, c_shape, c_ndim, output_coords);

  float max_val = FLT_MIN;
  u64 input_coords[MAX_NDIM];
  for (u64 i = 0; i < reduce_size; ++i) {
    for (u64 d = 0; d < a_ndim; ++d) {
      if (d == dim) {
        input_coords[d] = i;
      } else {
        input_coords[d] = output_coords[d > dim && !keepdim ? d - 1 : d];
      }
    }
    u64 input_offset = cuda_get_offset(input_coords, a_strides, a_ndim);
    max_val = fmaxf(max_val, a_data[input_offset]);
  }
  u64 output_offset = cuda_get_offset(output_coords, c_strides, c_ndim);
  c_data[output_offset] = max_val;
}

__global__ void max_cuda_backward_float_dim_contig_kernel(
    const float *dc_data, const u64 *dc_shape, const u64 *dc_strides,
    u64 dc_ndim, float *da_data, const float *a_data_fwd,
    const float *c_data_fwd, const u64 *da_shape, const u64 *da_strides,
    u64 da_ndim, u64 dim, u64 reduce_size, bool keepdim) {
  u64 input_idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (input_idx >= cuda_numel_from_shape(da_shape, da_ndim)) {
    return;
  }

  u64 input_coords[MAX_NDIM];
  cuda_linear_to_coords(input_idx, da_shape, da_ndim, input_coords);
  u64 input_offset = cuda_get_offset(input_coords, da_strides, da_ndim);

  u64 output_coords[MAX_NDIM];
  u64 output_dim_idx = 0;
  for (u64 d = 0; d < da_ndim; ++d) {
    if (d == dim) {
      if (keepdim) {
        output_coords[output_dim_idx++] = 0;
      } else {
        continue;
      }
    } else {
      output_coords[output_dim_idx++] = input_coords[d];
    }
  }
  u64 output_offset = cuda_get_offset(output_coords, dc_strides, dc_ndim);

  if (a_data_fwd[input_offset] == c_data_fwd[output_offset]) {
    da_data[input_offset] += dc_data[output_offset];
  }
}

__global__ void max_cuda_backward_float_dim_noncontig_kernel(
    const float *dc_data, const u64 *dc_shape, const u64 *dc_strides,
    u64 dc_ndim, float *da_data, const float *a_data_fwd,
    const float *c_data_fwd, const u64 *da_shape, const u64 *da_strides,
    u64 da_ndim, u64 dim, u64 reduce_size, bool keepdim) {
  u64 input_idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (input_idx >= cuda_numel_from_shape(da_shape, da_ndim)) {
    return;
  }

  u64 input_coords[MAX_NDIM];
  cuda_linear_to_coords(input_idx, da_shape, da_ndim, input_coords);
  u64 input_offset = cuda_get_offset(input_coords, da_strides, da_ndim);

  u64 output_coords[MAX_NDIM];
  u64 output_dim_idx = 0;
  for (u64 d = 0; d < da_ndim; ++d) {
    if (d == dim) {
      if (keepdim) {
        output_coords[output_dim_idx++] = 0;
      } else {
        continue;
      }
    } else {
      output_coords[output_dim_idx++] = input_coords[d];
    }
  }
  u64 output_offset = cuda_get_offset(output_coords, dc_strides, dc_ndim);

  if (a_data_fwd[input_offset] == c_data_fwd[output_offset]) {
    da_data[input_offset] += dc_data[output_offset];
  }
}

__global__ void max_cuda_backward_float_contig_kernel(const float *dc,
                                                      float *da,
                                                      const float *a_data_fwd,
                                                      const float *c_data_fwd,
                                                      u64 num_elements) {
  u64 idx = blockDim.x * blockIdx.x + threadIdx.x;
  u64 strides = blockDim.x * gridDim.x;
  float grad = dc[0];
  float max_val = c_data_fwd[0];

  for (u64 i = idx; i < num_elements; i += strides) {
    if (a_data_fwd[i] == max_val) {
      da[i] += grad;
    }
  }
}

__global__ void max_cuda_backward_float_noncontig_kernel(
    const float *dc, float *da, const float *a_data_fwd,
    const float *c_data_fwd, u64 *a_shape, u64 *a_strides, u64 a_ndim,
    u64 num_elements) {
  u64 idx = blockDim.x * blockIdx.x + threadIdx.x;
  u64 strides = blockDim.x * gridDim.x;

  u64 coords[MAX_NDIM];
  float grad = dc[0];
  float max_val = c_data_fwd[0];

  for (u64 i = idx; i < num_elements; i += strides) {
    cuda_linear_to_coords(i, a_shape, a_ndim, coords);
    u64 offset = cuda_get_offset(coords, a_strides, a_ndim);
    if (a_data_fwd[offset] == max_val) {
      da[offset] += grad;
    }
  }
}

void max_cuda_forward(const Tensor **inputs, Tensor *output, ...) {
  const Tensor *a = inputs[0];
  float *c_data = (float *)output->data;

  va_list args;
  va_start(args, output);
  u64 dim = va_arg(args, u64);
  bool keepdim = va_arg(args, int);
  va_end(args);

  if (dim == MAX_NDIM + 1) {
    u64 num_elements = numel(a);
    const u64 block_size = 256;
    const u64 grid_size =
        (num_elements + (block_size * 2) - 1) / (block_size * 2);
    const u64 shared_mem_size = block_size * sizeof(float);

    if (is_contiguous(a)) {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_forward_float_contig_kernel<block_size>
            <<<grid_size, block_size, shared_mem_size>>>((const float *)a->data,
                                                         c_data, num_elements);
        break;
      default:
        break;
      }
    } else {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_forward_float_noncontig_kernel<block_size>
            <<<grid_size, block_size, shared_mem_size>>>(
                (const float *)a->data, a->shape, a->strides, a->ndim, c_data,
                num_elements);
        break;
      default:
        break;
      }
    }
  } else {
    u64 reduce_size = a->shape[dim];
    u64 output_num_elements = numel(output);

    const u64 block_size = 256;
    const u64 grid_size = (output_num_elements + block_size - 1) / block_size;

    if (is_contiguous(a)) {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_forward_float_dim_contig_kernel<<<grid_size, block_size>>>(
            (const float *)a->data, a->shape, a->strides, a->ndim, c_data,
            output->shape, output->strides, output->ndim, dim, reduce_size,
            keepdim);
        break;
      default:
        break;
      }
    } else {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_forward_float_dim_noncontig_kernel<<<grid_size, block_size>>>(
            (const float *)a->data, a->shape, a->strides, a->ndim, c_data,
            output->shape, output->strides, output->ndim, dim, reduce_size,
            keepdim);
        break;
      default:
        break;
      }
    }
  }
}

void max_cuda_backward(Tensor **inputs, const Tensor *output, ...) {
  Tensor *a = inputs[0];
  const float *dc_data = (const float *)output->grad->data;
  float *da_data = (float *)a->grad->data;
  va_list args;
  va_start(args, output);
  u64 dim = va_arg(args, u64);
  bool keepdim = va_arg(args, int);
  va_end(args);

  const float *a_data_fwd = (const float *)a->data;
  const float *c_data_fwd = (const float *)output->data;

  if (dim == MAX_NDIM + 1) {
    u64 num_elements = numel(a);
    const u64 block_size = 256;
    const u64 grid_size = (num_elements + block_size - 1) / block_size;

    if (is_contiguous(a)) {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_backward_float_contig_kernel<<<grid_size, block_size>>>(
            dc_data, da_data, a_data_fwd, c_data_fwd, num_elements);
        break;
      default:
        break;
      }
    } else {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_backward_float_noncontig_kernel<<<grid_size, block_size>>>(
            dc_data, da_data, a_data_fwd, c_data_fwd, a->shape, a->strides,
            a->ndim, num_elements);
        break;
      default:
        break;
      }
    }
  } else {
    u64 reduce_size = a->shape[dim];
    u64 input_num_elements = numel(a);

    const u64 block_size = 256;
    const u64 grid_size = (input_num_elements + block_size - 1) / block_size;

    if (is_contiguous(a)) {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_backward_float_dim_contig_kernel<<<grid_size, block_size>>>(
            dc_data, output->shape, output->strides, output->ndim, da_data,
            a_data_fwd, c_data_fwd, a->shape, a->strides, a->ndim, dim,
            reduce_size, keepdim);
        break;
      default:
        break;
      }
    } else {
      switch (a->dtype) {
      case FLOAT32:
        max_cuda_backward_float_dim_noncontig_kernel<<<grid_size, block_size>>>(
            dc_data, output->shape, output->strides, output->ndim, da_data,
            a_data_fwd, c_data_fwd, a->shape, a->strides, a->ndim, dim,
            reduce_size, keepdim);
        break;
      default:
        break;
      }
    }
  }
}
